# -*- coding: utf-8 -*-
"""mnist_JosephHuang.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13OJ4fQct4ox5pFnBf6URfDnpTMhjRi4U
"""

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
import numpy as np
from google.colab import files

def TRAIN_SIZE(num):
    print ('Total Training Images in Dataset = ' + str(mnist.train.images.shape))
    print ('--------------------------------------------------')
    x_train = mnist.train.images[:num,:]
    print ('x_train Examples Loaded = ' + str(x_train.shape))
    y_train = mnist.train.labels[:num,:]
    print ('y_train Examples Loaded = ' + str(y_train.shape))
    print('')
    return x_train, y_train

def TEST_SIZE(num):
    print ('Total Test Examples in Dataset = ' + str(mnist.test.images.shape))
    print ('--------------------------------------------------')
    x_test = mnist.test.images[:num,:]
    print ('x_test Examples Loaded = ' + str(x_test.shape))
    y_test = mnist.test.labels[:num,:]
    print ('y_test Examples Loaded = ' + str(y_test.shape))
    return x_test, y_test

import tensorflow as tf
sess = tf.Session()

x = tf.placeholder(tf.float32, shape=[None, 784])

y_ = tf.placeholder(tf.float32, shape=[None, 10])

lr = tf.Variable(0.001, dtype=tf.float32, name='learning_rate')
keep_prob = tf.placeholder(tf.float32)

W = tf.Variable(tf.random_uniform([784, 500], -0.01, 0.01))
b = tf.Variable(tf.zeros([500]))
h0 = tf.nn.relu(tf.matmul(x, W) + b)
W2 = tf.Variable(tf.random_uniform([500, 10], -0.01, 0.01))  # add layer
b2 = tf.Variable(tf.zeros([10]))

print(x)


y = tf.nn.softmax(tf.matmul(h0, W2) + b2)

"""Adam optimiaer"""

x_train, y_train = TRAIN_SIZE(1000)
x_test, y_test = TEST_SIZE(1000)

cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y, labels =y_ ))

#cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))



train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y, labels = y_))

sess = tf.Session()
sess.run(tf.global_variables_initializer())
saver = tf.train.Saver()

def compute_accuracy(v_xs, v_ys):
    y_pre = sess.run(y, feed_dict={x: v_xs})
    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    result = sess.run(accuracy, feed_dict={x: v_xs, y_: v_ys})
    return result

for i in range(3000):
  
    batch_xs, batch_ys = mnist.train.next_batch(100)
    x1,y1=sess.run([train_step,accuracy], feed_dict={x: batch_xs, y_: batch_ys})
    #saver.save(sess, 'outputfile.txt')
    if i % 200 == 0:
        print('epoch # :' + str(i) +'\ttestset accuracy: '+str(compute_accuracy(x_test, y_test))+ '\ttrainset accuracy: '+ str(compute_accuracy(batch_xs, batch_ys))+ '\tloss: '+ str(sess.run(cross_entropy, {x: batch_xs, y_: batch_ys})))

print(b)

# weight1 = W.eval(sess)
# #print(weight1)
# np.savetxt("weight1.csv",weight1,delimiter=',')
# files.download("weight1.csv")

# weight2 = W2.eval(sess)
# #print(weight2)
# np.savetxt("weight2.csv",weight2,delimiter=',')
# files.download("weight2.csv")

# bias1 = b.eval(sess)
# #print(bias1)
# np.savetxt("bias1.csv",bias1,delimiter=',')
# files.download("bias1.csv")

# bias2 = b2.eval(sess)
# #print(bias2)
# np.savetxt("bias2.csv",bias2,delimiter=',')
# files.download("bias2.csv")

sess.close()

print(batch_xs)
print(len(batch_xs[0]))